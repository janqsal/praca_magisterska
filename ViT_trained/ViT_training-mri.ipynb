{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff33b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms import functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from vit_pytorch.vit import ViT, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a414d7-3f5c-4853-9a49-b0b03f07f178",
   "metadata": {},
   "source": [
    "### Ustawienia treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee9ad8a-5d39-49f5-9182-00b5a0202aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "lr = 3e-5\n",
    "gamma = 0.9\n",
    "seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2ee47f-f0a1-4733-9d66-18031c9b3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c04f85-7a55-4d39-a2f1-05a1da058dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02b7290-fc1a-4cc9-bddb-67fda643ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3080\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\torch\\cuda\\memory.py:444: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1100762a-5fda-46ac-807d-1dfdf23e0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\Jan\\SGH\\magisterka\\dane\\brain_tumor_mri_cropped'\n",
    "base_dataset = datasets.ImageFolder(root=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421fb2be-3a7a-4801-bfdb-9344921889f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(base_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd21d4f-35f1-4b1c-9b50-682c7988f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total = len(base_dataset)\n",
    "num_train = int(0.8 * num_total)\n",
    "num_val = int(0.1 * num_total)\n",
    "num_test = num_total - num_train - num_val  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e1445d5-4861-4375-aa56-6c158a24d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział indeksów datasetu na treningowy, walidacyjny i testowy\n",
    "indices = torch.randperm(len(base_dataset)).tolist()\n",
    "train_indices = indices[:num_train]\n",
    "val_indices = indices[num_train:num_train + num_val]\n",
    "test_indices = indices[num_train + num_val:]\n",
    "\n",
    "# Utworzenie podzbiorów z odpowiednimi indeksami\n",
    "train_dataset = Subset(base_dataset, train_indices)\n",
    "val_dataset = Subset(base_dataset, val_indices)\n",
    "test_dataset = Subset(base_dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d9b391-638e-4696-97ff-f4a574089f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacje dla zbioru treningowego\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa1c48d-9c8c-423f-96e4-78b6135babc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcje collate_fn dla każdego z zestawów danych\n",
    "def train_collate_fn(batch):\n",
    "    processed_batch = [(train_transform(x[0]), x[1]) for x in batch]\n",
    "    return torch.utils.data.dataloader.default_collate(processed_batch)\n",
    "\n",
    "def val_collate_fn(batch):\n",
    "    processed_batch = [(val_transform(x[0]), x[1]) for x in batch]\n",
    "    return torch.utils.data.dataloader.default_collate(processed_batch)\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    processed_batch = [(test_transform(x[0]), x[1]) for x in batch]\n",
    "    return torch.utils.data.dataloader.default_collate(processed_batch)\n",
    "\n",
    "# Utworzenie DataLoaderów z odpowiednimi transformacjami\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collate_fn)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "044cd584-ea7c-4d0f-a0ea-1b1a14f6ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    dim=768,\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=num_classes,\n",
    "    pool='cls',\n",
    "    channels=3,\n",
    "    emb_dropout=0.1,\n",
    "    depth=12,\n",
    "    heads=12,\n",
    "    dim_head=64,\n",
    "    mlp_dim=3072,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07999d09-4c6f-4208-a791-2c89eec01a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dc41d8d-3c65-42f3-b63c-be3fd20566c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def train_model_with_timing(train_loader, valid_loader, model, criterion, optimizer, epochs, device):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "\n",
    "        for data, label in train_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_train += label.size(0)\n",
    "            correct_train += (predicted == label).sum().item()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_accuracy = 100 * correct_train / total_train\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        total_val = 0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, label in valid_loader:\n",
    "                data, label = data.to(device), label.to(device)\n",
    "                val_output = model(data)\n",
    "                val_loss = criterion(val_output, label)\n",
    "                _, predicted = torch.max(val_output, 1)\n",
    "                total_val += label.size(0)\n",
    "                correct_val += (predicted == label).sum().item()\n",
    "                epoch_val_loss += val_loss.item()\n",
    "            \n",
    "        epoch_val_accuracy = 100 * correct_val / total_val\n",
    "        val_losses.append(epoch_val_loss / len(valid_loader))\n",
    "        val_accuracies.append(epoch_val_accuracy)\n",
    "\n",
    "        if epoch_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_accuracy\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"Epoka {epoch+1}/{epochs} - Strata: {epoch_loss:.4f}, Dokładność: {epoch_accuracy:.2f}%, \"\n",
    "              f\"Strata walidacyjna: {epoch_val_loss:.4f}, Dokładność walidacyjna: {epoch_val_accuracy:.2f}%, \"\n",
    "              f\"Czas trwania epoki: {epoch_end_time - epoch_start_time:.2f}s\")\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    print(f\"Całkowity czas treningu: {training_end_time - training_start_time:.2f}s\")\n",
    "\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "    }\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c80a3ee-1ee9-4c03-921e-6b5bde8ead4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoka 1/100 - Strata: 243.0042, Dokładność: 32.72%, Strata walidacyjna: 30.0443, Dokładność walidacyjna: 44.87%, Czas trwania epoki: 74.46s\n",
      "Epoka 2/100 - Strata: 221.9558, Dokładność: 42.83%, Strata walidacyjna: 24.4324, Dokładność walidacyjna: 52.56%, Czas trwania epoki: 72.65s\n",
      "Epoka 3/100 - Strata: 206.9829, Dokładność: 48.33%, Strata walidacyjna: 22.0741, Dokładność walidacyjna: 56.13%, Czas trwania epoki: 74.40s\n",
      "Epoka 4/100 - Strata: 185.8567, Dokładność: 55.38%, Strata walidacyjna: 16.9493, Dokładność walidacyjna: 70.51%, Czas trwania epoki: 72.72s\n",
      "Epoka 5/100 - Strata: 180.2030, Dokładność: 56.35%, Strata walidacyjna: 16.3021, Dokładność walidacyjna: 70.37%, Czas trwania epoki: 72.06s\n",
      "Epoka 6/100 - Strata: 166.0538, Dokładność: 61.37%, Strata walidacyjna: 14.6339, Dokładność walidacyjna: 75.64%, Czas trwania epoki: 72.51s\n",
      "Epoka 7/100 - Strata: 159.4939, Dokładność: 62.94%, Strata walidacyjna: 14.9976, Dokładność walidacyjna: 71.51%, Czas trwania epoki: 71.67s\n",
      "Epoka 8/100 - Strata: 156.9975, Dokładność: 63.94%, Strata walidacyjna: 15.8517, Dokładność walidacyjna: 69.52%, Czas trwania epoki: 72.09s\n",
      "Epoka 9/100 - Strata: 148.9575, Dokładność: 65.61%, Strata walidacyjna: 14.7432, Dokładność walidacyjna: 71.65%, Czas trwania epoki: 73.74s\n",
      "Epoka 10/100 - Strata: 147.4343, Dokładność: 65.57%, Strata walidacyjna: 14.0525, Dokładność walidacyjna: 75.64%, Czas trwania epoki: 71.66s\n",
      "Epoka 11/100 - Strata: 142.7087, Dokładność: 67.48%, Strata walidacyjna: 13.2882, Dokładność walidacyjna: 77.64%, Czas trwania epoki: 70.78s\n",
      "Epoka 12/100 - Strata: 139.7767, Dokładność: 67.34%, Strata walidacyjna: 14.1723, Dokładność walidacyjna: 73.22%, Czas trwania epoki: 70.60s\n",
      "Epoka 13/100 - Strata: 137.3399, Dokładność: 69.24%, Strata walidacyjna: 13.1746, Dokładność walidacyjna: 77.21%, Czas trwania epoki: 71.62s\n",
      "Epoka 14/100 - Strata: 137.6190, Dokładność: 67.71%, Strata walidacyjna: 11.9294, Dokładność walidacyjna: 80.20%, Czas trwania epoki: 72.84s\n",
      "Epoka 15/100 - Strata: 134.7837, Dokładność: 69.40%, Strata walidacyjna: 13.6866, Dokładność walidacyjna: 76.35%, Czas trwania epoki: 73.18s\n",
      "Epoka 16/100 - Strata: 131.5553, Dokładność: 70.74%, Strata walidacyjna: 13.1207, Dokładność walidacyjna: 76.50%, Czas trwania epoki: 73.17s\n",
      "Epoka 17/100 - Strata: 131.7218, Dokładność: 69.53%, Strata walidacyjna: 12.6462, Dokładność walidacyjna: 76.92%, Czas trwania epoki: 72.50s\n",
      "Epoka 18/100 - Strata: 130.8665, Dokładność: 70.17%, Strata walidacyjna: 11.3936, Dokładność walidacyjna: 81.62%, Czas trwania epoki: 76.14s\n",
      "Epoka 19/100 - Strata: 129.7273, Dokładność: 70.52%, Strata walidacyjna: 11.9937, Dokładność walidacyjna: 78.92%, Czas trwania epoki: 71.87s\n",
      "Epoka 20/100 - Strata: 126.0990, Dokładność: 71.84%, Strata walidacyjna: 12.5260, Dokładność walidacyjna: 77.92%, Czas trwania epoki: 71.59s\n",
      "Epoka 21/100 - Strata: 124.5150, Dokładność: 72.61%, Strata walidacyjna: 12.4541, Dokładność walidacyjna: 78.21%, Czas trwania epoki: 71.01s\n",
      "Epoka 22/100 - Strata: 123.4561, Dokładność: 72.48%, Strata walidacyjna: 11.9205, Dokładność walidacyjna: 80.06%, Czas trwania epoki: 71.38s\n",
      "Epoka 23/100 - Strata: 125.9021, Dokładność: 71.73%, Strata walidacyjna: 11.0227, Dokładność walidacyjna: 82.34%, Czas trwania epoki: 71.83s\n",
      "Epoka 24/100 - Strata: 119.3588, Dokładność: 73.30%, Strata walidacyjna: 13.1920, Dokładność walidacyjna: 75.93%, Czas trwania epoki: 72.83s\n",
      "Epoka 25/100 - Strata: 121.6616, Dokładność: 72.57%, Strata walidacyjna: 11.3430, Dokładność walidacyjna: 81.20%, Czas trwania epoki: 72.61s\n",
      "Epoka 26/100 - Strata: 119.2018, Dokładność: 73.41%, Strata walidacyjna: 11.5596, Dokładność walidacyjna: 82.05%, Czas trwania epoki: 72.95s\n",
      "Epoka 27/100 - Strata: 118.6812, Dokładność: 73.62%, Strata walidacyjna: 11.7888, Dokładność walidacyjna: 79.20%, Czas trwania epoki: 71.87s\n",
      "Epoka 28/100 - Strata: 117.2284, Dokładność: 72.98%, Strata walidacyjna: 12.7761, Dokładność walidacyjna: 78.35%, Czas trwania epoki: 71.88s\n",
      "Epoka 29/100 - Strata: 117.1298, Dokładność: 73.46%, Strata walidacyjna: 10.6760, Dokładność walidacyjna: 83.05%, Czas trwania epoki: 71.37s\n",
      "Epoka 30/100 - Strata: 115.4808, Dokładność: 73.62%, Strata walidacyjna: 11.0996, Dokładność walidacyjna: 82.62%, Czas trwania epoki: 71.61s\n",
      "Epoka 31/100 - Strata: 115.3250, Dokładność: 74.58%, Strata walidacyjna: 11.4609, Dokładność walidacyjna: 79.91%, Czas trwania epoki: 72.03s\n",
      "Epoka 32/100 - Strata: 113.3414, Dokładność: 74.26%, Strata walidacyjna: 10.9921, Dokładność walidacyjna: 80.77%, Czas trwania epoki: 71.72s\n",
      "Epoka 33/100 - Strata: 115.0615, Dokładność: 74.67%, Strata walidacyjna: 11.1732, Dokładność walidacyjna: 80.48%, Czas trwania epoki: 72.00s\n",
      "Epoka 34/100 - Strata: 116.3973, Dokładność: 74.46%, Strata walidacyjna: 11.4304, Dokładność walidacyjna: 80.63%, Czas trwania epoki: 72.22s\n",
      "Epoka 35/100 - Strata: 113.9960, Dokładność: 74.65%, Strata walidacyjna: 10.4424, Dokładność walidacyjna: 83.19%, Czas trwania epoki: 74.00s\n",
      "Epoka 36/100 - Strata: 115.4961, Dokładność: 74.05%, Strata walidacyjna: 11.6600, Dokładność walidacyjna: 80.63%, Czas trwania epoki: 73.80s\n",
      "Epoka 37/100 - Strata: 111.5781, Dokładność: 74.53%, Strata walidacyjna: 10.4104, Dokładność walidacyjna: 83.62%, Czas trwania epoki: 71.50s\n",
      "Epoka 38/100 - Strata: 111.2606, Dokładność: 75.04%, Strata walidacyjna: 10.3574, Dokładność walidacyjna: 83.05%, Czas trwania epoki: 71.40s\n",
      "Epoka 39/100 - Strata: 112.2586, Dokładność: 75.19%, Strata walidacyjna: 10.8265, Dokładność walidacyjna: 81.20%, Czas trwania epoki: 71.80s\n",
      "Epoka 40/100 - Strata: 110.3499, Dokładność: 75.38%, Strata walidacyjna: 12.1135, Dokładność walidacyjna: 79.06%, Czas trwania epoki: 72.18s\n",
      "Epoka 41/100 - Strata: 108.8581, Dokładność: 75.13%, Strata walidacyjna: 9.9966, Dokładność walidacyjna: 83.19%, Czas trwania epoki: 71.89s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_timing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m, in \u001b[0;36mtrain_model_with_timing\u001b[1;34m(train_loader, valid_loader, model, criterion, optimizer, epochs, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m total_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m correct_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     23\u001b[0m     data, label \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 247\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vit_pytorch\\lib\\site-packages\\PIL\\Image.py:3164\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixels \u001b[38;5;241m>\u001b[39m MAX_IMAGE_PIXELS:\n\u001b[0;32m   3157\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3158\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels) exceeds limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_IMAGE_PIXELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould be decompression bomb DOS attack.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3160\u001b[0m             DecompressionBombWarning,\n\u001b[0;32m   3161\u001b[0m         )\n\u001b[1;32m-> 3164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(fp, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3166\u001b[0m \u001b[38;5;124;03m    Opens and identifies the given image file.\u001b[39;00m\n\u001b[0;32m   3167\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3191\u001b[0m \u001b[38;5;124;03m    :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\u001b[39;00m\n\u001b[0;32m   3192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = train_model_with_timing(train_loader, valid_loader, model, criterion, optimizer, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8e095-b15b-4383-8530-9f13f8a246dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=test_loader.dataset.dataset.classes)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d36012-4430-4558-b305-bc58d900e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_loss_accuracy_plots_with_timing(history):\n",
    "    tr_acc = history['train_accuracies']\n",
    "    tr_loss = history['train_losses']\n",
    "    val_acc = history['val_accuracies']\n",
    "    val_loss = history['val_losses']\n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    Epochs = range(1, len(tr_acc) + 1)\n",
    "    loss_label = f'Najlepsza epoka (strata) = {index_loss + 1}'\n",
    "    acc_label = f'Najlepsza epoka (dokładność) = {index_acc + 1}'\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r-', label='Zbiór uczący - strata')\n",
    "    plt.plot(Epochs, val_loss, 'g-', label='Zbiór walidacyjny - strata')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)\n",
    "    plt.title('Funkcja straty dla zbioru uczącego oraz walidacyjnego')\n",
    "    plt.xlabel('Epoki')\n",
    "    plt.ylabel('Strata')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r-', label='Dokładność zbioru uczącego')\n",
    "    plt.plot(Epochs, val_acc, 'g-', label='Dokładność zbioru walidacyjnego')\n",
    "    plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)\n",
    "    plt.title('Dokładność dla zbioru uczącego oraz walidacyjnego')\n",
    "    plt.xlabel('Epoki')\n",
    "    plt.ylabel('Dokładność [%]')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a0c33-242b-4416-abac-75e1bdcf4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_plots_with_timing(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5425fa6-4c2b-47b9-94b9-4c387b32a928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
